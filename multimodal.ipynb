{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import AutoModel  # Electra, BERT 모델을 동적으로 불러오기 위해\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "books_df = pd.read_csv('../../data/books.csv')\n",
    "users_df = pd.read_csv('../../data/users.csv')\n",
    "train_ratings_df = pd.read_csv('../../data/train_ratings.csv')\n",
    "test_ratings_df = pd.read_csv('../../data/test_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['age'] = pd.to_numeric(users_df['age'], errors='coerce')\n",
    "median_age = users_df['age'].median()\n",
    "users_df['age'] = users_df['age'].fillna(median_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['year_of_publication'] = pd.to_numeric(books_df['year_of_publication'], errors='coerce')\n",
    "median_year = books_df['year_of_publication'].median()\n",
    "books_df['year_of_publication'] = books_df['year_of_publication'].fillna(median_year).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 병합\n",
    "train_data = train_ratings_df.merge(books_df, on='isbn').merge(users_df, on='user_id')\n",
    "test_data = test_ratings_df.merge(books_df, on='isbn').merge(users_df, on='user_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User의 평균 평점 계산\n",
    "user_avg_rating = train_data.groupby('user_id')['rating'].mean().reset_index(name='user_mean_rating')\n",
    "\n",
    "# Book의 평균 평점 계산\n",
    "book_avg_rating = train_data.groupby('isbn')['rating'].mean().reset_index(name='book_mean_rating')\n",
    "\n",
    "# 원래 데이터프레임에 user와 book의 평균 평점 합치기\n",
    "train_data = train_data.merge(user_avg_rating, on='user_id')\n",
    "train_data = train_data.merge(book_avg_rating, on='isbn')\n",
    "\n",
    "# 테스트 데이터에 훈련 데이터의 평균 평점 적용\n",
    "test_data = test_data.merge(user_avg_rating, on='user_id', how='left')\n",
    "test_data = test_data.merge(book_avg_rating, on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_url</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_path</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "      <th>user_mean_rating</th>\n",
       "      <th>book_mean_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>4</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>timmins, ontario, canada</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67544</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>7</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>toronto, ontario, canada</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123629</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>kingston, ontario, canada</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200273</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>comber, ontario, canada</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210926</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>9</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>guelph, ontario, canada</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>6.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>278843</td>\n",
       "      <td>0743525493</td>\n",
       "      <td>7</td>\n",
       "      <td>The Motley Fool's What To Do with Your Money N...</td>\n",
       "      <td>David Gardner</td>\n",
       "      <td>2002</td>\n",
       "      <td>Simon &amp; Schuster Audio</td>\n",
       "      <td>http://images.amazon.com/images/P/0743525493.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/0743525493.01.THUMBZZZ.jpg</td>\n",
       "      <td>pismo beach, california, usa</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>278851</td>\n",
       "      <td>067161746X</td>\n",
       "      <td>6</td>\n",
       "      <td>The Bachelor Home Companion: A Practical Guide...</td>\n",
       "      <td>P.J. O'Rourke</td>\n",
       "      <td>1987</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>http://images.amazon.com/images/P/067161746X.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Humor']</td>\n",
       "      <td>A tongue-in-cheek survival guide for single pe...</td>\n",
       "      <td>images/067161746X.01.THUMBZZZ.jpg</td>\n",
       "      <td>dallas, texas, usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>278851</td>\n",
       "      <td>0884159221</td>\n",
       "      <td>7</td>\n",
       "      <td>Why stop?: A guide to Texas historical roadsid...</td>\n",
       "      <td>Claude Dooley</td>\n",
       "      <td>1985</td>\n",
       "      <td>Lone Star Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0884159221.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/0884159221.01.THUMBZZZ.jpg</td>\n",
       "      <td>dallas, texas, usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>278851</td>\n",
       "      <td>0912333022</td>\n",
       "      <td>7</td>\n",
       "      <td>The Are You Being Served? Stories: 'Camping In...</td>\n",
       "      <td>Jeremy Lloyd</td>\n",
       "      <td>1997</td>\n",
       "      <td>Kqed Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0912333022.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>These hilarious stories by the creator of publ...</td>\n",
       "      <td>images/0912333022.01.THUMBZZZ.jpg</td>\n",
       "      <td>dallas, texas, usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>278851</td>\n",
       "      <td>1569661057</td>\n",
       "      <td>10</td>\n",
       "      <td>Dallas Street Map Guide and Directory, 2000 Ed...</td>\n",
       "      <td>Mapsco</td>\n",
       "      <td>1999</td>\n",
       "      <td>American Map Corporation</td>\n",
       "      <td>http://images.amazon.com/images/P/1569661057.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/1569661057.01.THUMBZZZ.jpg</td>\n",
       "      <td>dallas, texas, usa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        isbn  rating  \\\n",
       "0             8  0002005018       4   \n",
       "1         67544  0002005018       7   \n",
       "2        123629  0002005018       8   \n",
       "3        200273  0002005018       8   \n",
       "4        210926  0002005018       9   \n",
       "...         ...         ...     ...   \n",
       "306790   278843  0743525493       7   \n",
       "306791   278851  067161746X       6   \n",
       "306792   278851  0884159221       7   \n",
       "306793   278851  0912333022       7   \n",
       "306794   278851  1569661057      10   \n",
       "\n",
       "                                               book_title  \\\n",
       "0                                            Clara Callan   \n",
       "1                                            Clara Callan   \n",
       "2                                            Clara Callan   \n",
       "3                                            Clara Callan   \n",
       "4                                            Clara Callan   \n",
       "...                                                   ...   \n",
       "306790  The Motley Fool's What To Do with Your Money N...   \n",
       "306791  The Bachelor Home Companion: A Practical Guide...   \n",
       "306792  Why stop?: A guide to Texas historical roadsid...   \n",
       "306793  The Are You Being Served? Stories: 'Camping In...   \n",
       "306794  Dallas Street Map Guide and Directory, 2000 Ed...   \n",
       "\n",
       "                 book_author  year_of_publication                 publisher  \\\n",
       "0       Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
       "1       Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
       "2       Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
       "3       Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
       "4       Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
       "...                      ...                  ...                       ...   \n",
       "306790         David Gardner                 2002    Simon & Schuster Audio   \n",
       "306791         P.J. O'Rourke                 1987              Pocket Books   \n",
       "306792         Claude Dooley                 1985           Lone Star Books   \n",
       "306793          Jeremy Lloyd                 1997                Kqed Books   \n",
       "306794                Mapsco                 1999  American Map Corporation   \n",
       "\n",
       "                                                  img_url language  \\\n",
       "0       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "2       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "3       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "4       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "...                                                   ...      ...   \n",
       "306790  http://images.amazon.com/images/P/0743525493.0...      NaN   \n",
       "306791  http://images.amazon.com/images/P/067161746X.0...       en   \n",
       "306792  http://images.amazon.com/images/P/0884159221.0...      NaN   \n",
       "306793  http://images.amazon.com/images/P/0912333022.0...       en   \n",
       "306794  http://images.amazon.com/images/P/1569661057.0...      NaN   \n",
       "\n",
       "             category                                            summary  \\\n",
       "0       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "1       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "2       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "3       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "4       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "...               ...                                                ...   \n",
       "306790            NaN                                                NaN   \n",
       "306791      ['Humor']  A tongue-in-cheek survival guide for single pe...   \n",
       "306792            NaN                                                NaN   \n",
       "306793    ['Fiction']  These hilarious stories by the creator of publ...   \n",
       "306794            NaN                                                NaN   \n",
       "\n",
       "                                 img_path                      location   age  \\\n",
       "0       images/0002005018.01.THUMBZZZ.jpg      timmins, ontario, canada  34.0   \n",
       "1       images/0002005018.01.THUMBZZZ.jpg      toronto, ontario, canada  30.0   \n",
       "2       images/0002005018.01.THUMBZZZ.jpg     kingston, ontario, canada  34.0   \n",
       "3       images/0002005018.01.THUMBZZZ.jpg       comber, ontario, canada  34.0   \n",
       "4       images/0002005018.01.THUMBZZZ.jpg       guelph, ontario, canada  34.0   \n",
       "...                                   ...                           ...   ...   \n",
       "306790  images/0743525493.01.THUMBZZZ.jpg  pismo beach, california, usa  28.0   \n",
       "306791  images/067161746X.01.THUMBZZZ.jpg            dallas, texas, usa  33.0   \n",
       "306792  images/0884159221.01.THUMBZZZ.jpg            dallas, texas, usa  33.0   \n",
       "306793  images/0912333022.01.THUMBZZZ.jpg            dallas, texas, usa  33.0   \n",
       "306794  images/1569661057.01.THUMBZZZ.jpg            dallas, texas, usa  33.0   \n",
       "\n",
       "        user_mean_rating  book_mean_rating  \n",
       "0               4.428571          6.857143  \n",
       "1               7.285714          6.857143  \n",
       "2               8.000000          6.857143  \n",
       "3               8.000000          6.857143  \n",
       "4               8.400000          6.857143  \n",
       "...                  ...               ...  \n",
       "306790          8.000000          7.000000  \n",
       "306791          5.833333          6.000000  \n",
       "306792          5.833333          7.000000  \n",
       "306793          5.833333          7.000000  \n",
       "306794          5.833333         10.000000  \n",
       "\n",
       "[306795 rows x 16 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 특징 선택\n",
    "features = ['user_id', 'isbn', 'book_title', 'book_author', 'year_of_publication', 'publisher', \n",
    "            'age', 'img_path', 'summary', 'user_mean_rating', 'book_mean_rating']\n",
    "train_data = train_data[features + ['rating']]\n",
    "test_data = test_data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리를 위한 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리를 위한 변환 정의\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_818904/1826618647.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])\n"
     ]
    }
   ],
   "source": [
    "# 정규화 \n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['year_of_publication', 'age', 'user_mean_rating', 'book_mean_rating']\n",
    "train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data[numerical_features] = scaler.transform(test_data[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WDN 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide & Deep Network 모델 구현\n",
    "class WideAndDeepModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, text_model_name=\"google/electra-small-discriminator\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Wide 파트 (단순 합산)\n",
    "        self.wide = nn.Linear(num_numerical_features, 1)\n",
    "\n",
    "        # Deep 파트 - 이미지 인코더\n",
    "        self.image_encoder = models.resnet18(pretrained=True)\n",
    "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, 128)\n",
    "        \n",
    "        # Deep 파트 - 텍스트 인코더\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_embedding_dim = 256 if \"electra\" in text_model_name else 768\n",
    "        self.text_fc = nn.Linear(text_embedding_dim, 128)\n",
    "        \n",
    "        # Deep 파트 - 정형 데이터 인코더\n",
    "        self.numerical_encoder = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128)\n",
    "        )\n",
    "        \n",
    "        # Deep 파트 - 퓨전\n",
    "        self.deep_fusion = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Wide와 Deep 결합을 위한 최종 예측 층\n",
    "        self.final_layer = nn.Linear(128 + 1, 1)\n",
    "\n",
    "    def forward(self, image, text_ids, text_mask, numerical_features, wide_features):\n",
    "        # Wide 파트 출력\n",
    "        y_wide = self.wide(wide_features).squeeze(1)\n",
    "        \n",
    "        # Deep 파트 - 이미지 인코딩\n",
    "        image_features = self.image_encoder(image)\n",
    "        \n",
    "        # Deep 파트 - 텍스트 인코딩\n",
    "        text_output = self.text_encoder(input_ids=text_ids, attention_mask=text_mask)\n",
    "        if hasattr(text_output, \"pooler_output\"):\n",
    "            text_features = self.text_fc(text_output.pooler_output)\n",
    "        else:\n",
    "            text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        # Deep 파트 - 정형 데이터 인코딩\n",
    "        numerical_features = self.numerical_encoder(numerical_features)\n",
    "        \n",
    "        # Deep 파트 - 이미지, 텍스트, 정형 데이터 특징 결합\n",
    "        combined_features = torch.cat([image_features, text_features, numerical_features], dim=1)\n",
    "        y_deep = self.deep_fusion(combined_features)\n",
    "\n",
    "        # Wide와 Deep 출력 결합\n",
    "        # final_features = torch.cat([y_wide, y_deep], dim=1)\n",
    "        # rating = self.final_layer(final_features)\n",
    "        # Wide와 Deep 출력 결합\n",
    "        final_features = torch.cat([y_wide.unsqueeze(1), y_deep], dim=1)\n",
    "        rating = self.final_layer(final_features)\n",
    "\n",
    "        \n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, numerical_features, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.numerical_features = numerical_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # 텍스트 처리\n",
    "        text = f\"{row['book_title']} {row['book_author']} {row['summary']}\"\n",
    "\n",
    "        text = ' '.join([str(item) if pd.notna(item) else '' for item in text.split()])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # 이미지 처리\n",
    "        # 이미지 경로 설정 시 중복되는 'images/' 부분 제거\n",
    "        image_filename = row['img_path'].replace('images/', '')  # 'images/' 부분 제거\n",
    "        image_path = os.path.join('/data/ephemeral/home/data/images', image_filename)\n",
    "        #image_path = os.path.join('/data/ephemeral/home/data/images', row['img_path'])\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = image_transform(image)\n",
    "\n",
    "        # 수치형 특징\n",
    "        # numerical = torch.tensor([row[feature] for feature in numerical_features], dtype=torch.float)\n",
    "        numerical = torch.tensor([row[feature] for feature in self.numerical_features], dtype=torch.float)\n",
    "\n",
    "\n",
    "        # 레이블 (학습 데이터의 경우)\n",
    "        if 'rating' in row:\n",
    "            label = torch.tensor(row['rating'], dtype=torch.float)\n",
    "        else:\n",
    "            label = torch.tensor(0, dtype=torch.float)  # 테스트 데이터의 경우 더미 값\n",
    "\n",
    "        return {\n",
    "            'text_ids': encoding['input_ids'].flatten(),\n",
    "            'text_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'numerical': numerical,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_1, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = BookDataset(train_data, tokenizer)\n",
    "# val_dataset = BookDataset(val_data, tokenizer)\n",
    "# test_dataset = BookDataset(test_data, tokenizer)\n",
    "\n",
    "train_dataset = BookDataset(train_data, tokenizer, numerical_features=numerical_features)\n",
    "val_dataset = BookDataset(val_data, tokenizer, numerical_features=numerical_features)\n",
    "test_dataset = BookDataset(test_data, tokenizer, numerical_features=numerical_features)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalBookRatingModel(nn.Module):\n",
    "    def __init__(self, num_numerical_features, text_model_name=\"google/electra-small-discriminator\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 이미지 인코더\n",
    "        self.image_encoder = models.resnet18(pretrained=True)\n",
    "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, 128)\n",
    "        \n",
    "        # 텍스트 인코더\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        \n",
    "        # 텍스트 임베딩 차원을 모델에 따라 동적으로 설정\n",
    "        text_embedding_dim = 256 if \"electra\" in text_model_name else 768\n",
    "        self.text_fc = nn.Linear(text_embedding_dim, 128)\n",
    "        \n",
    "        # 정형 데이터 인코더\n",
    "        self.numerical_encoder = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128)\n",
    "        )\n",
    "        \n",
    "        # 멀티모달 퓨전\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, text_ids, text_mask, numerical_features):\n",
    "        # 이미지 인코딩\n",
    "        image_features = self.image_encoder(image)\n",
    "        \n",
    "        # 텍스트 인코딩\n",
    "        text_output = self.text_encoder(input_ids=text_ids, attention_mask=text_mask)\n",
    "        \n",
    "        # Electra와 BERT 모델을 구분하여 텍스트 임베딩 생성\n",
    "        if hasattr(text_output, \"pooler_output\"):\n",
    "            text_features = self.text_fc(text_output.pooler_output)\n",
    "        else:\n",
    "            text_features = self.text_fc(text_output.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        # 정형 데이터 인코딩\n",
    "        numerical_features = self.numerical_encoder(numerical_features)\n",
    "        \n",
    "        # 특징 결합\n",
    "        combined_features = torch.cat([image_features, text_features, numerical_features], dim=1)\n",
    "        \n",
    "        # 평점 예측\n",
    "        rating = self.fusion(combined_features)\n",
    "        \n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WDN / 2-layer MLP 중 하나 선택하여 model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "# model = MultimodalBookRatingModel(num_numerical_features=len(numerical_features)) # WDN 아닌 기존 2-layer MLP \n",
    "model = WideAndDeepModel(num_numerical_features=len(numerical_features)) # WDN\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WDN / 2-layer MLP 중 하나 선택하여 model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text_ids = batch['text_ids'].to(device)\n",
    "        text_mask = batch['text_mask'].to(device)\n",
    "        image = batch['image'].to(device)\n",
    "        numerical = batch['numerical'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # wide_features를 numerical로 전달\n",
    "        # outputs = model(image, text_ids, text_mask, numerical) # WDN 이 아닌 기존 Multimodal을 사용할 경우 \n",
    "        outputs = model(image, text_ids, text_mask, numerical, wide_features=numerical) # WDN\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            text_ids = batch['text_ids'].to(device)\n",
    "            text_mask = batch['text_mask'].to(device)\n",
    "            image = batch['image'].to(device)\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # wide_features를 numerical로 전달\n",
    "            # outputs = model(image, text_ids, text_mask, numerical) # WDN 이 아닌 기존 Multimodal\n",
    "            outputs = model(image, text_ids, text_mask, numerical, wide_features=numerical) # WDN\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 2.2492, Val Loss: 2.0420\n",
      "Epoch 2/10, Train Loss: 2.0258, Val Loss: 2.0109\n",
      "Epoch 3/10, Train Loss: 2.0153, Val Loss: 2.0381\n",
      "Epoch 4/10, Train Loss: 2.0091, Val Loss: 2.0672\n",
      "Epoch 5/10, Train Loss: 2.0040, Val Loss: 2.0525\n",
      "Epoch 6/10, Train Loss: 2.0001, Val Loss: 2.0889\n",
      "Epoch 7/10, Train Loss: 1.9960, Val Loss: 1.9908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m----> 7\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[53], line 30\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, criterion, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     31\u001b[0m         text_ids \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtext_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m         text_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtext_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[47], line 38\u001b[0m, in \u001b[0;36mBookDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m#image_path = os.path.join('/data/ephemeral/home/data/images', row['img_path'])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m image \u001b[39m=\u001b[39m image_transform(image)\n\u001b[1;32m     40\u001b[0m \u001b[39m# 수치형 특징\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# numerical = torch.tensor([row[feature] for feature in numerical_features], dtype=torch.float)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m numerical \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([row[feature] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumerical_features], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "model.eval()\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        text_ids = batch['text_ids'].to(device)\n",
    "        text_mask = batch['text_mask'].to(device)\n",
    "        image = batch['image'].to(device)\n",
    "        numerical = batch['numerical'].to(device)\n",
    "        \n",
    "        # outputs = model(image, text_ids, text_mask, numerical) # WDN 이 아닌 기존 Multimodal\n",
    "        outputs = model(image, text_ids, text_mask, numerical, wide_features=numerical) # WDN\n",
    "        predictions.extend(outputs.cpu().numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과를 테스트 데이터프레임에 추가\n",
    "test_data['rating'] = predictions\n",
    "test_data['rating'] = test_data['rating'].fillna(test_data['rating'].mean()) # 결측치는 평균값으로 보간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "submission = test_data[['user_id', 'isbn', 'rating']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"예측이 완료되었습니다. 결과는 'submission.csv' 파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
